// Copyright (c) 2019-2023 Simons Foundation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0.txt
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Authors: Miguel Morales, Olivier Parcollet, Nils Wentzell

/**
 * @file
 * @brief Provides a generic interface to the BLAS `gemv` routine.
 */

#pragma once

#include "./interface/cxx_interface.hpp"
#include "./tools.hpp"
#include "../concepts.hpp"
#include "../macros.hpp"
#include "../mem/address_space.hpp"
#include "../traits.hpp"

#ifndef NDA_HAVE_DEVICE
#include "../device.hpp"
#endif

#include <tuple>

namespace nda::blas {

  /**
   * @brief Multiply a matrix by a vector and add it to another vector.
   *
   * @details Generic nda::blas::gemv implementation for types not supported by BLAS/LAPACK.
   *
   * @tparam A Matrix type.
   * @tparam X Vector type.
   * @tparam Y Vector type.
   * @param alpha Scalar constant.
   * @param a Input matrix of size m by n.
   * @param x Input vector of size n.
   * @param beta Scalar constant.
   * @param y Input/Output vector of size m.
   */
  template <typename A, typename X, typename Y>
  void gemv_generic(get_value_t<A> alpha, A const &a, X const &x, get_value_t<A> beta, Y &y) {
    EXPECTS(a.extent(1) == x.extent(0));
    EXPECTS(a.extent(0) == y.extent(0));
    for (int i = 0; i < a.extent(0); ++i) {
      y(i) = beta * y(i);
      for (int k = 0; k < a.extent(1); ++k) y(i) += alpha * a(i, k) * x(k);
    }
  }

  /**
   * @brief Interface to the BLAS `gemv` routine.
   *
   * @details This function performs one of the matrix-vector operations
   * - \f$ /mathbf{y} \leftarrow \alpha /mathbf{A} /mathbf{x} + \beta /mathbf{y} \f$,
   * - \f$ /mathbf{y} \leftarrow \alpha /mathbf{A}^T /mathbf{x} + \beta /mathbf{y} \f$,
   * - \f$ /mathbf{y} \leftarrow \alpha /mathbf{A}^H /mathbf{x} + \beta /mathbf{y} \f$,
   * where \f$ \alpha \f$ and \f$ \beta \f$ are scalars, \f$ /mathbf{x} \f$ and \f$ /mathbf{y} \f$
   * are vectors and \f$ /mathbf{A} \f$ is an m by n matrix.
   *
   * @tparam A nda::Matrix type.
   * @tparam X nda::MemoryVector type.
   * @tparam Y nda::MemoryVector type.
   * @param alpha Scalar constant.
   * @param a Input matrix of size m by n.
   * @param x Input vector of size n.
   * @param beta Scalar constant.
   * @param y Input/Output vector of size m.
   */
  template <Matrix A, MemoryVector X, MemoryVector Y>
    requires((MemoryMatrix<A> or is_conj_array_expr<A>) and have_same_value_type_v<A, X, Y> and is_blas_lapack_v<get_value_t<A>>)
  void gemv(get_value_t<A> alpha, A const &a, X const &x, get_value_t<A> beta, Y &&y) { // NOLINT (temporary views are allowed here)
    // get underlying matrix in case it is given as a lazy expression
    auto to_mat = []<Matrix Z>(Z const &z) -> decltype(auto) {
      if constexpr (is_conj_array_expr<Z>)
        return std::get<0>(z.a);
      else
        return z;
    };
    auto &mat = to_mat(a);

    // compile-time checks
    using mat_type = decltype(mat);
    static_assert(mem::have_compatible_addr_space<mat_type, X, Y>);

    // runtime checks
    EXPECTS(mat.extent(1) == x.extent(0));
    EXPECTS(mat.extent(0) == y.extent(0));
    EXPECTS(mat.indexmap().min_stride() == 1);
    EXPECTS(x.indexmap().min_stride() == 1);
    EXPECTS(y.indexmap().min_stride() == 1);

    // gather parameters for gemv call
    static constexpr bool conj_A = is_conj_array_expr<A>;
    char op_a                    = get_op<conj_A, /* transpose = */ !has_F_layout<mat_type>>;
    auto [m, n]                  = mat.shape();
    if constexpr (has_C_layout<mat_type>) std::swap(m, n);

    if constexpr (mem::have_device_compatible_addr_space<mat_type, X, Y>) {
#if defined(NDA_HAVE_DEVICE)
      device::gemv(op_a, m, n, alpha, mat.data(), get_ld(mat), x.data(), x.indexmap().strides()[0], beta, y.data(), y.indexmap().strides()[0]);
#else
      compile_error_no_gpu();
#endif
    } else {
      f77::gemv(op_a, m, n, alpha, mat.data(), get_ld(mat), x.data(), x.indexmap().strides()[0], beta, y.data(), y.indexmap().strides()[0]);
    }
  }

} // namespace nda::blas
